---
title: "Assignment 2"
author: "Vamshee Deepak Goud Katta"
date: "10/2/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

## 1. Inserting Data and Libraries

# Reading the UniversalBank csv file and inserting approproate libraries

```{r}

rm(list=ls())
library(class)
library(caret)
library(ISLR)
library(dummies)
library(tidyr)
library(dplyr)
library(ggplot2)

UBank_data <- read.csv("UniversalBank.csv")

```

## 2. Data Cleaning and Splitting

# Removing the ID and Zipcode columns from the Bank_data dataset
# Converting Education column into factor and creating a dummy database of Bank_data
#Moving personal.loan column to the first column position

```{r}

library(caret)
library(class)
library(ISLR)
library(tidyr)
library(dummies)
library(dplyr)
library(ggplot2)

UBank_data$Education = as.factor(UBank_data$Education)


UBank_data_d = dummy.data.frame(select(UBank_data, -c(ZIP.Code,ID)))

UBank_data_d$Mortgage = as.factor(UBank_data_d$Mortgage)
UBank_data_d$Income = as.factor(UBank_data_d$Income)


n <- which(colnames(UBank_data_d)=="Personal.Loan")
Personal.Loan <- UBank_data_d$Personal.Loan
UBank_data_d <- cbind(Personal.Loan, UBank_data_d[,-n])
head(UBank_data_d)

UBank_data_d

```

## 3. Data Partition
# Partitioning the UniversalBank dataset into Training and validation sets

```{r}

set.seed(123)
Index_Train <- createDataPartition(UBank_data_d$Personal.Loan, p=0.6, list = FALSE) 
# 60% of data is taken as Training Data

Train <- UBank_data_d[Index_Train,]
Validation <- UBank_data_d[-Index_Train,] 
# Rest of the data is taken as Validation Data

summary(Train)
summary(Validation)

```

## 4. Solving Q1 using given constraints

#Creating Test data from Q1 in Assignment
# Normalizing the Training Data
# Applying KNN Model with k=1

```{r}

# Copying original data
Train_norm <- Train
Validation_norm <- Validation
UBD_norm <- UBank_data_d
Test1 <- data.frame(40, 10, 84, 2, 2, 0, 1, 0, 0, 0, 0, 1, 1)

norm_values <- preProcess(Train_norm[,2:14], method = c("center", "scale"))
Train_norm[,2:14] <- predict(norm_values, Train_norm[,2:14])
Validation_norm[,2:14] <- predict(norm_values, Validation_norm[,2:14])

summary(Train_norm)
var(Train_norm[,2:14])
summary(Validation_norm)
var(Validation_norm[,2:14])

Q1_KNN <- knn(Train_norm[,2:14], Test1, cl = Train_norm[,1], k=1, prob = 0.5)
Q1_KNN
row.names(Train_norm)[attr(Q1_KNN, "Q1_KNN.index")]

```

# Training the KNN Model using the train() function along with preProcess
# Combining training and validation datasets and renormalizing data
# Expanding search grid to appropriate K value to gain highest possible accuracy

```{r}

set.seed(123)
Model <- train(Personal.Loan~Income+Mortgage, data =UBD_norm, method = "knn")
UBD_norm_Train <- predict(UBank_data_norm, Train)
UBD_norm_Validation <- predict(UBank_data_norm, Validation)

summary(UBD_norm_Train)
Summary(UBD_norm_Validation)

```

## 6. Finding the best value of K

```{r}

accuracy.df <- data.frame(k = seq(1, 14, 1), accuracy = rep(0, 14))
for(i in 1:14) {
                  knn <- knn(Train_norm, Validation_norm, cl = Train$Personal.Loan, k = i)
                  accuracy[i, 2] <- confusionMatrix(knn, Validation$Personal.Loan)$overall[1] 
                }
accuracy.df
which.max( (accuracy.df$accuracy) )

```

# Repartitioning data into 5:3:2

```{r}

set.seed(123)
Index_Train2 <- createDataPartition(UBank_data_d$Personal.Loan, p=0.5, list = FALSE) 
# 50% of data is taken as Training Data

Train2 <- UBank_data_d[Index_Train2,]
Validation.test <- UBank_data_d[-Index_Train2,] 
# Rest of the data is taken as Validation and Test Data
Index_Validation <- createDataPartition(Validation.test$Personal.Loan, p=0.6, list = FALSE)
# 30% of total or 60% of remaining data is taken as Validation Data
Validation2 <- Validation.test[Index_Validation,]
Test2 <- Validation.test[-Index_Train2,]

summary(Train2)
summary(Validation2)
summary(Test2)

```

# Creating Confusion Matrix

```{r}

Train_Predictors <- Train_norm[,2:14]
Test_Predictors <- Validation[,2:14]

Train_labels <-Train_norm[,1]
Test_labels  <-Validation_norm[,1]

Predicted_Test_labels <- knn(Train_Predictors, Test_Predictors, cl=Train_labels, k=1, prob = TRUE)
library(gmodels)
CrossTable(x=Test_labels,y=Predicted_Test_labels, prop.chisq = FALSE)

```
# Normalizing the data

```{r}

Train_norm2 <- Train2
Validation_norm2 <- Validation2

norm_values2 <- preProcess(Train_norm2[,2:14], method = c("center", "scale"))
Train_norm2[,2:14] <- predict(norm_values2, Train_norm2[,2:14])
Validation_norm2[,2:14] <- predict(norm_values2, Validation_norm2[,2:14])

summary(Train_norm2)
var(Train_norm2[,2:14])
summary(Validation_norm2)
var(Validation_norm2[,2:14])

```

# Combining Train2 and Validation2 datasets to renormalize and apply to Test2 Dataset

```{r}

ForTest_norm2 <- rbind.data.frame(Train_norm2, Validation_norm2, deparse.level = 1, make.row.names = T, stringsAsFactors = F, factor.exclude = T)
ForTest_norm2

```

#Re-normalizing the combined data

```{r}

norm_values3 <- preProcess(ForTest_norm2[,2:14], method = c("center", "scale"))
Test_norm2[,2:14] <- predict(norm_values3, ForTest_norm2[,2:14])

summary(Test_norm2)
var(Test_norm2[,2:14])

```

# Creating Confusion Matrix 2

```{r}

Train_Predictors <- Train_norm2[,2:14]
Test_Predictors <- Validation2[,2:14]

Train_labels2 <-Train_norm2[,1]
Test_labels2  <-Validation_norm2[,1]

Predicted_Test_labels2 <- knn(Train_Predictors, Test_Predictors, cl=Train_labels2, k=1, prob = TRUE)
library(gmodels)
CrossTable(x=Test_labels2, y=Predicted_Test_labels2, prop.chisq = FALSE)

```

# Applying KNN Method with the best K

```{r}

Q5_KNN <- knn(Train_norm2[,2:14], Test2[,2:14], cl = Train_norm2[,1], k=3, prob = TRUE)
Q5_KNN
row.names(Train_norm2)[attr(Q5_KNN, "Q5_KNN.index")]

```

# Creating Confusion Matrix 3

```{r}

Train_Predictors <- ForTest_norm2[,2:14]
Test_Predictors <- Test_norm2[,2:14]

Train_labels3 <-ForTest_norm2[,1]
Test_labels3  <-Test_norm2[,1]

Predicted_Test_labels3 <- knn(Train_Predictors, Test_Predictors, cl=Train_labels3, k=1, prob = TRUE)
library(gmodels)
CrossTable(x=Test_labels3, y=Predicted_Test_labels3, prop.chisq = FALSE)

```



